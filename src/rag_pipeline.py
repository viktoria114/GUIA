# src/rag_pipeline.py
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory, ChatMessageHistory
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from config import OR_TOKEN
import os
import traceback

# 1. CARGAR DOCUMENTOS
def cargar_documentos():
    try:
        loader = DirectoryLoader(
            "./docs",
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True
        )
        documentos = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=100,
            length_function=len
        )
        chunks = text_splitter.split_documents(documentos)
        print(f"‚úÖ Se cargaron {len(chunks)} chunks de documentos")
        return chunks
    except Exception as e:
        print(f"‚ùå Error cargando documentos: {e}")
        return []

# 2. CREAR VECTORSTORE
def crear_vectorstore(chunks):
    try:
        embeddings = HuggingFaceEmbeddings(
               model_name="BAAI/bge-m3",               # üëà nombre correcto
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': True},
    cache_folder="./hf_models",             # opcional: para guardar local
        )
        print("‚úÖ HuggingFaceEmbeddings inicializado correctamente.")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="../chroma_db"
        )
        print("‚úÖ Vectorstore creado y guardado")
        return vectorstore
    except Exception as e:
        print(f"‚ùå Error creando vectorstore: {e}")
        traceback.print_exc()
        return None

# 3. CREAR LLM
def crear_llm():
    try:
        llm = ChatOpenAI(
            model="deepseek/deepseek-chat-v3.1:free",
            temperature=0.1,
            max_tokens=1024,
            openai_api_key=OR_TOKEN,
            openai_api_base="https://openrouter.ai/api/v1"
        )
        print("‚úÖ LLM configurado con OpenRouter Deepseek")
        return llm
    except Exception as e:
        print(f"‚ùå Error configurando LLM: {e}")
        return None

# 4. MEMORIA
def crear_memoria():
    return ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
        chat_memory=ChatMessageHistory()
        
    )

# 5. CADENA DE QA CON CONTEXTO UNLaR
def crear_qa_chain(vectorstore, llm):
    try:
        memory = crear_memoria()
        historial = memory.chat_memory.messages
        print(f"‚úÖ Memoria: {historial}")


        # Prompt base con contexto de la UNLaR
        system_prompt = """
Eres un asistente virtual dise√±ado para ayudar a estudiantes de la Universidad Nacional de La Rioja (UNLaR).
Tu funci√≥n principal es responder preguntas y brindar asistencia sobre temas acad√©micos, administrativos y de la vida universitaria en la UNLaR.

=== OBJETIVOS ===
- Brindar respuestas precisas y fieles al contenido de los documentos proporcionados.
- Incluir toda la informaci√≥n relevante encontrada en las fuentes, priorizando la claridad y la utilidad para el estudiante.
- Mantener una comunicaci√≥n cercana y amigable, usando un tono cordial y accesible para un p√∫blico joven.
- Responder siempre en el mismo idioma en que el usuario formule la pregunta.

=== REGLAS DE RESPUESTA ===
1. Presenta las respuestas en forma de resumen explicativo. 
   - Usa p√°rrafos para explicaciones generales.
   - Emplea listas numeradas o con vi√±etas solo cuando la informaci√≥n se preste a enumeraciones claras.
2. No uses citas textuales a menos que el usuario solicite expl√≠citamente el texto exacto.
3. Si no encuentras informaci√≥n suficiente en los documentos, responde de manera clara que no cuentas con esa informaci√≥n. 
   - Sugiere amablemente al usuario consultar la p√°gina oficial de la UNLaR, contactar a la universidad por sus medios oficiales o acudir presencialmente a las oficinas para confirmar.
4. Si la pregunta no est√° relacionada con la UNLaR o su contexto educativo/administrativo, informa al usuario que est√° fuera de tu prop√≥sito principal.
5. Adapta el nivel de detalle seg√∫n la pregunta:
   - Para consultas breves, responde de forma concisa pero completa.
   - Para consultas m√°s complejas, responde de forma m√°s detallada, sin omitir informaci√≥n relevante.
6. Nunca pidas datos personales ni sugieras que el usuario los proporcione.
7. Si el usuario no solicita informaci√≥n espec√≠fica, responde cort√©smente sin forzar una b√∫squeda en los documentos.

=== CONTEXTO DISPONIBLE ===
La siguiente informaci√≥n ha sido extra√≠da de documentos oficiales de la UNLaR. Utiliza este contexto para responder a las preguntas del usuario de la mejor manera posible.
{context}

=== INSTRUCCI√ìN ===
Pregunta del usuario: {question}

Tu respuesta debe seguir fielmente estas reglas y objetivos.
"""

        prompt = PromptTemplate(
            template=system_prompt,
            input_variables=["context", "question"]  # OBLIGATORIO para StuffDocumentsChain
        )

        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 3, "lambda_mult": 0.7}),
            memory=memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": prompt},
            verbose=True
        )
        print("‚úÖ Cadena RAG creada con contexto UNLaR")
        return qa_chain
    except Exception as e:
        print(f"‚ùå Error creando cadena RAG: {e}")
        traceback.print_exc()
        return None

# 6. PROBAR SISTEMA
def probar_sistema():
    print("üß™ Probando sistema...")
    chunks = cargar_documentos()
    if not chunks: return False

    vectorstore = crear_vectorstore(chunks)
    if not vectorstore: return False

    llm = crear_llm()
    if not llm: return False

    qa_chain = crear_qa_chain(vectorstore, llm)
    if not qa_chain: return False

    try:
        respuesta = qa_chain.invoke({"question": "¬øCuales son los alcances del titulo de Ingeniero en sistemas?"})
        for doc in respuesta["source_documents"]:
         print(doc.metadata.get("source"))
        print("‚úÖ Sistema funcionando:", respuesta["answer"][:300] + "...")
        return True
    except Exception as e:
        print("‚ùå Error en prueba:", str(e))
        traceback.print_exc()
        return False
    
    

if __name__ == "__main__":
    probar_sistema()
