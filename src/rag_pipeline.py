# src/rag_pipeline.py
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory, ChatMessageHistory
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from config import OR_TOKEN
import os
import traceback

# 1. CARGAR DOCUMENTOS
def cargar_documentos():
    try:
        loader = DirectoryLoader(
            "./docs",
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True
        )
        documentos = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=100,
            length_function=len
        )
        chunks = text_splitter.split_documents(documentos)
        print(f" Se cargaron {len(chunks)} chunks de documentos")
        return chunks
    except Exception as e:
        print(f" Error cargando documentos: {e}")
        return []

# 2. CREAR VECTORSTORE
def crear_vectorstore(chunks):
    try:
        embeddings = HuggingFaceEmbeddings(
               model_name="BAAI/bge-m3",               # 游녣 nombre correcto
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True},
    cache_folder="./hf_models",             # opcional: para guardar local
        )
        print(" HuggingFaceEmbeddings inicializado correctamente.")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="../chroma_db"
        )
        print(" Vectorstore creado y guardado")
        return vectorstore
    except Exception as e:
        print(f" Error creando vectorstore: {e}")
        traceback.print_exc()
        return None

# 3. CREAR LLM
def crear_llm():
    try:
        llm = ChatOpenAI(
            model="deepseek/deepseek-chat-v3.1:free",
            temperature=0.1,
            max_tokens=1024,
            openai_api_key=OR_TOKEN,
            openai_api_base="https://openrouter.ai/api/v1"
        )
        print(" LLM configurado con OpenRouter Deepseek")
        return llm
    except Exception as e:
        print(f" Error configurando LLM: {e}")
        return None

# 4. MEMORIA
def crear_memoria():
    return ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
        chat_memory=ChatMessageHistory()
        
    )

# 5. CADENA DE QA CON CONTEXTO UNLaR
def crear_qa_chain(vectorstore, llm):
    try:
        memory = crear_memoria()
        historial = memory.chat_memory.messages
        print(f" Memoria: {historial}")


        # Prompt base con contexto de la UNLaR
        system_prompt = """
Eres un asistente virtual dise침ado para ayudar a estudiantes de la Universidad Nacional de La Rioja (UNLaR).
Tu funci칩n principal es responder preguntas y brindar asistencia sobre temas acad칠micos, administrativos y de la vida universitaria en la UNLaR.
=== OBJETIVOS ===
- Brindar respuestas precisas y fieles al contenido de los documentos proporcionados.
- Incluir toda la informaci칩n relevante encontrada en las fuentes, priorizando la claridad y la utilidad para el estudiante.
- Mantener una comunicaci칩n cercana y amigable, usando un tono cordial y accesible para un p칰blico joven.
- Detecta autom치ticamente el idioma en que el usuario escribe su pregunta, y responde en ese mismo idioma, sin traducirlo al espa침ol por defecto.

=== REGLAS DE RESPUESTA ===
1. Presenta las respuestas en forma de resumen explicativo. 
   - Usa p치rrafos para explicaciones generales.
   - Emplea listas numeradas o con vi침etas solo cuando la informaci칩n se preste a enumeraciones claras.
2. No uses citas textuales a menos que el usuario solicite expl칤citamente el texto exacto.
3. Si no encuentras informaci칩n suficiente en los documentos, responde de manera clara que no cuentas con esa informaci칩n. 
   - Sugiere amablemente al usuario consultar la p치gina oficial de la UNLaR, contactar a la universidad por sus medios oficiales o acudir presencialmente a las oficinas para confirmar.
4. Si la pregunta no est치 relacionada con la UNLaR o su contexto educativo/administrativo, informa al usuario que est치 fuera de tu prop칩sito principal.
5. Adapta el nivel de detalle seg칰n la pregunta:
   - Para consultas breves, responde de forma concisa pero completa.
   - Para consultas m치s complejas, responde de forma m치s detallada, sin omitir informaci칩n relevante.
6. Nunca pidas datos personales ni sugieras que el usuario los proporcione.
7. Si el usuario no solicita informaci칩n espec칤fica, responde cort칠smente sin forzar una b칰squeda en los documentos.

=== CONTEXTO DISPONIBLE ===
La siguiente informaci칩n ha sido extra칤da de documentos oficiales de la UNLaR. Utiliza este contexto para responder a las preguntas del usuario de la mejor manera posible.
{context}

=== INSTRUCCI칍N ===
Pregunta del usuario: {question}

Tu respuesta debe seguir fielmente estas reglas y objetivos.
"""

        prompt = PromptTemplate(
            template=system_prompt,
            input_variables=["context", "question"]  # OBLIGATORIO para StuffDocumentsChain
        )

        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 3, "lambda_mult": 0.7}),
            memory=memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": prompt},
            verbose=True
        )
        print(" Cadena RAG creada con contexto UNLaR")
        return qa_chain
    except Exception as e:
        print(f" Error creando cadena RAG: {e}")
        traceback.print_exc()
        return None

# 6. PROBAR SISTEMA
def probar_sistema():
    print(" Probando sistema...")
    chunks = cargar_documentos()
    if not chunks: return False

    vectorstore = crear_vectorstore(chunks)
    if not vectorstore: return False

    llm = crear_llm()
    if not llm: return False

    qa_chain = crear_qa_chain(vectorstore, llm)
    if not qa_chain: return False

    try:
        respuesta = qa_chain.invoke({"question": "쮺uales son los alcances del titulo de Ingeniero en sistemas?"})
        for doc in respuesta["source_documents"]:
         print(doc.metadata.get("source"))
        print(" Sistema funcionando:", respuesta["answer"][:300] + "...")
        return True
    except Exception as e:
        print(" Error en prueba:", str(e))
        traceback.print_exc()
        return False
    
    

if __name__ == "__main__":
    probar_sistema()
